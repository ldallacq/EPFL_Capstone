{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL DATASET CREATION\n",
    "\n",
    "For the Final Dataset creation, I took a step-by-step and iterative approach. I started working on one persona at the time and understand its unique way of speaking. I then re-worked back all the personas and cleaned them in the same way (e.g. I have started with Biden and moved on to Kim Kardashian to go back again to re-clean Biden). \n",
    "\n",
    "This approach helped to understand the complexity of cleaning Twitter data, since each individual has a very specific way of communicating, uses a specific slang and has specificities to be taken into consideration when applying a similar cleaning across datasets. \n",
    "\n",
    "For each persona, this are overall the steps I have followed for the cleaning and features engineering and their explanation: \n",
    "\n",
    "* dropping duplicated Tweets with same URL (tweet_id)\n",
    "* counting the number of words\n",
    "* counting the total number of characters\n",
    "* finding/counting/removing all the hashtags used\n",
    "* finding/counting/removing all the @mentions used\n",
    "* finding/counting/removing all the emojis used\n",
    "* dropping all tweets considered as Retweets (retweet were identified as having indication of RT + @mention) **\n",
    "* removing websites indication (anything that would start with a call to action towards a website with 'http')\n",
    "* counting upper case words \n",
    "* setting the cleaned test to lower case \n",
    "\n",
    "In the end I have a text which is free from retweets, hashtags, emojis, at-mentions and it is ready to be manpulated with NLP techniques. \n",
    "\n",
    "\n",
    "** this was due to the fact that for example Biden and Trump were using the call to action to their audience by using the overall communication 'RT this post if you support me\", which I decided to keep in the analysis as it is a way of doing politics. I did not clean cleaning retweets only having the indication RT but only the ones which were clearly indicating that the persona was retweeting someone else tweets (with the @mention). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import emoji\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden initial dataset size: 6185\n"
     ]
    }
   ],
   "source": [
    "# loading dataset\n",
    "bid = pd.read_csv('biden_with_likes.csv')\n",
    "print('Biden initial dataset size:', len(bid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden dataset size without URL duplicates: 6065\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates with URL\n",
    "bid = bid.drop_duplicates(subset = 'tweet_id')\n",
    "print('Biden dataset size without URL duplicates:', len(bid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of words used in a tweet\n",
    "bid['total_words'] = bid['tweet'].str.split().str.len()\n",
    "\n",
    "# counting the number of characters used in a tweet\n",
    "bid['total_charact'] = bid['tweet'].apply(lambda x: len(x))\n",
    "\n",
    "# creating a column with all the hashtags used\n",
    "bid['hashtag'] = bid['tweet'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)) \n",
    "\n",
    "# creating a column with the number of hashtags used\n",
    "bid['num_hashtags'] = bid['hashtag'].apply(lambda x: len(x))\n",
    "\n",
    "# creating a column with @mentions only \n",
    "bid['mentions'] = bid['tweet'].apply(lambda x: re.findall(r'@[A-Za-z0-9]+', x))\n",
    "\n",
    "# counting the @mentions only: \n",
    "bid['num_mentions'] = bid['mentions'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all emojis \n",
    "def extract_emojis(s):\n",
    "    return ''.join(c for c in s if c in emoji.UNICODE_EMOJI )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count the emojis \n",
    "\n",
    "def split_count(text):\n",
    "    total_emoji = []\n",
    "    data = re.findall(r'\\X',text)\n",
    "    flag = False\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):  \n",
    "            total_emoji += [word] # total_emoji is a list of all emojis\n",
    "    return Counter(total_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the emojis, counting them and making an overall count\n",
    "\n",
    "bid['emojis'] = bid['tweet'].apply(lambda x: extract_emojis(x))\n",
    "bid['counter_emojis'] = bid['tweet'].apply(lambda x: split_count(x))\n",
    "bid['num_emojis'] = bid.counter_emojis.apply(lambda x : sum(x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emojis</th>\n",
       "      <th>counter_emojis</th>\n",
       "      <th>num_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>üá∫üá∏üá∫üá∏üá∫üá∏</td>\n",
       "      <td>{'üá∫üá∏': 3}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>üá∫üá∏üá∫üá∏üá∫üá∏</td>\n",
       "      <td>{'üá∫üá∏': 3}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>‚úÖ‚úÖ‚úÖ</td>\n",
       "      <td>{'‚úÖ': 3}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>‚úÖ‚úÖ‚úÖ</td>\n",
       "      <td>{'‚úÖ': 3}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>üî¥üî¥üî¥</td>\n",
       "      <td>{'üî¥': 3}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>‚úÖ‚úÖ‚úÖ</td>\n",
       "      <td>{'‚úÖ': 3}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>‚úÖ‚úÖ‚úÖ</td>\n",
       "      <td>{'‚úÖ': 3}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>‚úÖ‚úÖ‚úÖ</td>\n",
       "      <td>{'‚úÖ': 3}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      emojis counter_emojis  num_emojis\n",
       "999   üá∫üá∏üá∫üá∏üá∫üá∏      {'üá∫üá∏': 3}           3\n",
       "1027  üá∫üá∏üá∫üá∏üá∫üá∏      {'üá∫üá∏': 3}           3\n",
       "1634     ‚úÖ‚úÖ‚úÖ       {'‚úÖ': 3}           3\n",
       "1657     ‚úÖ‚úÖ‚úÖ       {'‚úÖ': 3}           3\n",
       "2209     üî¥üî¥üî¥       {'üî¥': 3}           3\n",
       "2226     ‚úÖ‚úÖ‚úÖ       {'‚úÖ': 3}           3\n",
       "2268     ‚úÖ‚úÖ‚úÖ       {'‚úÖ': 3}           3\n",
       "2562     ‚úÖ‚úÖ‚úÖ       {'‚úÖ': 3}           3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bid[bid.num_emojis >1][['emojis', 'counter_emojis', 'num_emojis']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLEANING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column with RT and @mentions\n",
    "# the reason is that user might asks his audience to retweet his tweets (e.g. \"RT this if you..\")\n",
    "\n",
    "bid['RT'] = bid['tweet'].apply(lambda x: re.findall(r'RT @(\\w+):', x))\n",
    "\n",
    "#bid['RT'] = bid['tweet'].apply(lambda x: re.findall(r'^(RT)( @\\w*)?[: ]', x))\n",
    "#bid['RT'] = bid['tweet'].apply(lambda x: re.findall(r'(?<!RT\\s)@\\S+', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Retweets: 27\n"
     ]
    }
   ],
   "source": [
    "# checking the number of Retweets\n",
    "print('total Retweets:',len(bid[(bid['RT'].str.len() > 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden dataset size without retweets: 6038\n"
     ]
    }
   ],
   "source": [
    "# dropping what appear to clearly be Retweets (RT + @mentions)\n",
    "bid = bid[(bid['RT'].str.len() == 0)]\n",
    "print('Biden dataset size without retweets:', len(bid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanind tweets from hashtags\n",
    "bid['cleaned'] = bid['tweet'].apply(lambda x: ' '.join(re.sub(r'\\B#\\w*[a-zA-Z]+\\w*',\" \", x).split()))\n",
    "\n",
    "# cleanind tweets from @mentions\n",
    "bid['cleaned'] = bid['cleaned'].apply(lambda x: ' '.join(re.sub(r'@[A-Za-z0-9]+',\" \", x).split()))\n",
    "\n",
    "# cleaning tweets from websites \n",
    "bid['cleaned'] = bid['cleaned'].apply(lambda x: ' '.join(re.sub(r'\\b(?:https?://)?(?:(?i:[a-z]+\\.)+)[^\\s,]+\\b',\" \", x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "# removing emojis \n",
    "bid['cleaned'] = bid['cleaned'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation\n",
    "punctuations = '!\"‚Äú‚Äù#$%&\\'‚Äô‚Äò()*+‚Äî-.‚Äì,‚Äì/:;<=>?@[\\\\]^_`{|}~‚Ñ¢¬Æ¬©¬π‚Åâ'\n",
    "bid['cleaned'] = bid['cleaned'].apply(lambda x: ''.join([i for i in x if not i in punctuations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of words in caps lock in the cleaned text\n",
    "# removing the \"I\" as it is counted as a capital word but it shouldn't\n",
    "bid['num_upper_words'] = bid['cleaned'].apply(lambda x: sum([y.isupper() if y not in ['I', 'U', 'R'] else False for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting everything in lower character\n",
    "bid['final'] = bid['cleaned'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only relevant columns\n",
    "bid = bid[['date', \n",
    "           'likes',\n",
    "           'retweets',\n",
    "           'tweet',\n",
    "           'tweet_id',\n",
    "           'total_words',\n",
    "           'total_charact', \n",
    "           'hashtag', \n",
    "           'num_hashtags', \n",
    "           'mentions', \n",
    "           'num_mentions', \n",
    "           'emojis', \n",
    "           'counter_emojis',\n",
    "           'num_emojis',\n",
    "           'num_upper_words',\n",
    "           'cleaned',\n",
    "           'final'\n",
    "            ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kim Kardashian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kim initial dataset size: 29696\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "kim = pd.read_csv('kim_with_likes.csv')\n",
    "print('Kim initial dataset size:', len(kim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kim dataset size without URL duplicates: 29115\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates with URL\n",
    "kim = kim.drop_duplicates(subset = 'tweet_id')\n",
    "print('Kim dataset size without URL duplicates:', len(kim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of words used in a tweet\n",
    "kim['total_words'] = kim['tweet'].str.split().str.len()\n",
    "\n",
    "# counting the number of characters used in a tweet\n",
    "kim['total_charact'] = kim['tweet'].apply(lambda x: len(x))\n",
    "\n",
    "# creating a column with all the hashtags used\n",
    "kim['hashtag'] = kim['tweet'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)) \n",
    "\n",
    "# creating a column with the number of hashtags used\n",
    "kim['num_hashtags'] = kim['hashtag'].apply(lambda x: len(x))\n",
    "\n",
    "# creating a column with @mentions only \n",
    "kim['mentions'] = kim['tweet'].apply(lambda x: re.findall(r'@[A-Za-z0-9]+', x))\n",
    "\n",
    "# counting the @mentions only: \n",
    "kim['num_mentions'] = kim['mentions'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the emojis, counting them and making an overall count\n",
    "kim['emojis'] = kim['tweet'].apply(lambda x: extract_emojis(x))\n",
    "kim['counter_emojis'] = kim['tweet'].apply(lambda x: split_count(x))\n",
    "kim['num_emojis'] = kim.counter_emojis.apply(lambda x : sum(x.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column with RT and @mentions\n",
    "# the reason is that user might asks his audience to retweet his tweets (e.g. \"RT this if you..\")\n",
    "\n",
    "kim['RT'] = kim['tweet'].apply(lambda x: re.findall(r'RT @(\\w+):', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Retweets: 1834\n"
     ]
    }
   ],
   "source": [
    "# checking how many retweets Kim did\n",
    "print('total Retweets:',len(kim[(kim['RT'].str.len() > 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kim dataset size without retweets: 27281\n"
     ]
    }
   ],
   "source": [
    "# dropping what appear to clearly be Retweets (RT + @mentions)\n",
    "kim = kim[(kim['RT'].str.len() == 0)]\n",
    "print('Kim dataset size without retweets:', len(kim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanind tweets from hashtags\n",
    "kim['cleaned'] = kim['tweet'].apply(lambda x: ' '.join(re.sub(r'\\B#\\w*[a-zA-Z]+\\w*',\" \", x).split()))\n",
    "\n",
    "# cleanind tweets from @mentions\n",
    "#kim['cleaned'] = kim['cleaned'].apply(lambda x: ' '.join(re.sub(r'@(\\w+):',\" \", x).split()))\n",
    "kim['cleaned'] = kim['cleaned'].apply(lambda x: ' '.join(re.sub(r'@[A-Za-z0-9]+',\" \", x).split()))\n",
    "\n",
    "# cleaning tweets from websites \n",
    "kim['cleaned'] = kim['cleaned'].apply(lambda x: ' '.join(re.sub(r'\\b(?:https?://)?(?:(?i:[a-z]+\\.)+)[^\\s,]+\\b',\" \", x).split()))\n",
    "\n",
    "# removing emojis \n",
    "kim['cleaned'] = kim['cleaned'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "# removing punctuation\n",
    "kim['cleaned'] = kim['cleaned'].apply(lambda x: ''.join([i for i in x if not i in punctuations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of words in caps lock in the cleaned text\n",
    "# removing the \"I\" as it is counted as a capital word but it shouldn't\n",
    "kim['num_upper_words'] = kim['cleaned'].apply(lambda x: sum([y.isupper() if y not in ['I', 'U', 'R'] else False for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting everything in lower character\n",
    "kim['final'] = kim['cleaned'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only relevant columns\n",
    "kim = kim[['date', \n",
    "           'likes',\n",
    "           'retweets',\n",
    "           'tweet',\n",
    "           'tweet_id',\n",
    "           'total_words',\n",
    "           'total_charact', \n",
    "           'hashtag', \n",
    "           'num_hashtags', \n",
    "           'mentions', \n",
    "           'num_mentions', \n",
    "           'emojis', \n",
    "           'counter_emojis',\n",
    "           'num_emojis',\n",
    "           'num_upper_words',\n",
    "           'cleaned',\n",
    "           'final'\n",
    "            ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump initial dataset size: 55090\n"
     ]
    }
   ],
   "source": [
    "# loading dataset\n",
    "trump = pd.read_csv('trump_with_likes.csv')\n",
    "print('Trump initial dataset size:', len(trump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing column names for ease of usage\n",
    "trump.rename(columns={'id':'tweet_id'}, inplace=True)\n",
    "trump.rename(columns={'text':'tweet'}, inplace=True)\n",
    "trump.rename(columns={'favorites':'likes'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump dataset size without URL duplicates: 55090\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates with URL\n",
    "trump = trump.drop_duplicates(subset = 'tweet_id')\n",
    "print('Trump dataset size without URL duplicates:', len(trump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f    54050\n",
       "t     1040\n",
       "Name: isDeleted, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleted tweets \n",
    "trump.isDeleted.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only the non-deleted data\n",
    "trump = trump[trump.isDeleted == 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f    45121\n",
       "t     8929\n",
       "Name: isRetweet, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retweets \n",
    "trump.isRetweet.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only the non-retweeted data\n",
    "trump = trump[trump.isRetweet == 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump after first basic cleaning (deleted & retweets) dataset size: 45121\n"
     ]
    }
   ],
   "source": [
    "trump.shape\n",
    "print('Trump after first basic cleaning (deleted & retweets) dataset size:', len(trump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of words used in a tweet\n",
    "trump['total_words'] = trump['tweet'].str.split().str.len()\n",
    "\n",
    "# counting the number of characters used in a tweet\n",
    "trump['total_charact'] = trump['tweet'].apply(lambda x: len(x))\n",
    "\n",
    "# creating a column with all the hashtags used\n",
    "trump['hashtag'] = trump['tweet'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)) \n",
    "\n",
    "# creating a column with the number of hashtags used\n",
    "trump['num_hashtags'] = trump['hashtag'].apply(lambda x: len(x))\n",
    "\n",
    "# creating a column with @mentions only \n",
    "trump['mentions'] = trump['tweet'].apply(lambda x: re.findall(r'@[A-Za-z0-9]+', x))\n",
    "\n",
    "# counting the @mentions only: \n",
    "trump['num_mentions'] = trump['mentions'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the emojis, counting them and making an overall count\n",
    "trump['emojis'] = trump['tweet'].apply(lambda x: extract_emojis(x))\n",
    "trump['counter_emojis'] = trump['tweet'].apply(lambda x: split_count(x))\n",
    "trump['num_emojis'] = trump.counter_emojis.apply(lambda x : sum(x.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column with RT and @mentions\n",
    "# the reason is that user might asks his audience to retweet his tweets (e.g. \"RT this if you..\")\n",
    "\n",
    "trump['RT'] = trump['tweet'].apply(lambda x: re.findall(r'RT @(\\w+):', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Retweets: 44\n"
     ]
    }
   ],
   "source": [
    "# checking how many retweets Kim did\n",
    "print('total Retweets:',len(trump[(trump['RT'].str.len() > 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump dataset size without retweets: 45077\n"
     ]
    }
   ],
   "source": [
    "# dropping what appear to clearly be Retweets (RT + @mentions)\n",
    "trump = trump[(trump['RT'].str.len() == 0)]\n",
    "print('Trump dataset size without retweets:', len(trump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanind tweets from hashtags\n",
    "trump['cleaned'] = trump['tweet'].apply(lambda x: ' '.join(re.sub(r'\\B#\\w*[a-zA-Z]+\\w*',\" \", x).split()))\n",
    "\n",
    "# cleanind tweets from @mentions\n",
    "trump['cleaned'] = trump['cleaned'].apply(lambda x: ' '.join(re.sub(r'@[A-Za-z0-9]+',\" \", x).split()))\n",
    "\n",
    "# cleaning tweets from websites \n",
    "trump['cleaned'] = trump['cleaned'].apply(lambda x: ' '.join(re.sub(r'\\b(?:https?://)?(?:(?i:[a-z]+\\.)+)[^\\s,]+\\b',\" \", x).split()))\n",
    "\n",
    "# removing emojis \n",
    "trump['cleaned'] = trump['cleaned'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "# removing punctuation\n",
    "trump['cleaned'] = trump['cleaned'].apply(lambda x: ''.join([i for i in x if not i in punctuations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of words in caps lock in the cleaned text\n",
    "# removing the \"I\" as it is counted as a capital word but it shouldn't\n",
    "trump['num_upper_words'] = trump['cleaned'].apply(lambda x: sum([y.isupper() if y not in ['I', 'U', 'R'] else False for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting everything in lower character\n",
    "trump['final'] = trump['cleaned'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only relevant columns\n",
    "trump = trump[['date', \n",
    "           'likes',\n",
    "           'retweets',\n",
    "           'tweet',\n",
    "           'tweet_id',\n",
    "           'total_words',\n",
    "           'total_charact', \n",
    "           'hashtag', \n",
    "           'num_hashtags', \n",
    "           'mentions', \n",
    "           'num_mentions', \n",
    "           'emojis', \n",
    "           'counter_emojis',\n",
    "           'num_emojis',\n",
    "           'num_upper_words',\n",
    "           'cleaned',\n",
    "           'final'\n",
    "            ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE POPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pope initial dataset size: 3444\n"
     ]
    }
   ],
   "source": [
    "# loading dataset\n",
    "pope = pd.read_csv('pope_with_likes.csv')\n",
    "print('Pope initial dataset size:', len(pope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pope dataset size without URL duplicates: 2878\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates with URL\n",
    "pope = pope.drop_duplicates(subset = 'tweet_id')\n",
    "print('Pope dataset size without URL duplicates:', len(pope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of words used in a tweet\n",
    "pope['total_words'] = pope['tweet'].str.split().str.len()\n",
    "\n",
    "# counting the number of characters used in a tweet\n",
    "pope['total_charact'] = pope['tweet'].apply(lambda x: len(x))\n",
    "\n",
    "# creating a column with all the hashtags used\n",
    "pope['hashtag'] = pope['tweet'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)) \n",
    "\n",
    "# creating a column with the number of hashtags used\n",
    "pope['num_hashtags'] = pope['hashtag'].apply(lambda x: len(x))\n",
    "\n",
    "# creating a column with @mentions only \n",
    "pope['mentions'] = pope['tweet'].apply(lambda x: re.findall(r'@[A-Za-z0-9]+', x))\n",
    "\n",
    "# counting the @mentions only: \n",
    "pope['num_mentions'] = pope['mentions'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the emojis, counting them and making an overall count\n",
    "pope['emojis'] = pope['tweet'].apply(lambda x: extract_emojis(x))\n",
    "pope['counter_emojis'] = pope['tweet'].apply(lambda x: split_count(x))\n",
    "pope['num_emojis'] = pope.counter_emojis.apply(lambda x : sum(x.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column with RT and @mentions\n",
    "# the reason is that user might asks his audience to retweet his tweets (e.g. \"RT this if you..\")\n",
    "\n",
    "pope['RT'] = pope['tweet'].apply(lambda x: re.findall(r'RT @(\\w+):', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Retweets: 0\n"
     ]
    }
   ],
   "source": [
    "# checking how many retweets the Pope did\n",
    "print('total Retweets:',len(pope[(pope['RT'].str.len() > 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pope dataset size without retweets: 2878\n"
     ]
    }
   ],
   "source": [
    "# dropping what appear to clearly be Retweets (RT + @mentions)\n",
    "pope = pope[(pope['RT'].str.len() == 0)]\n",
    "print('Pope dataset size without retweets:', len(pope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanind tweets from hashtags\n",
    "pope['cleaned'] = pope['tweet'].apply(lambda x: ' '.join(re.sub(r'\\B#\\w*[a-zA-Z]+\\w*',\" \", x).split()))\n",
    "\n",
    "# cleanind tweets from @mentions\n",
    "pope['cleaned'] = pope['cleaned'].apply(lambda x: ' '.join(re.sub(r'@[A-Za-z0-9]+',\" \", x).split()))\n",
    "\n",
    "# cleaning tweets from websites \n",
    "pope['cleaned'] = pope['cleaned'].apply(lambda x: ' '.join(re.sub(r'\\b(?:https?://)?(?:(?i:[a-z]+\\.)+)[^\\s,]+\\b',\" \", x).split()))\n",
    "\n",
    "# removing emojis \n",
    "pope['cleaned'] = pope['cleaned'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "# removing punctuation\n",
    "pope['cleaned'] = pope['cleaned'].apply(lambda x: ''.join([i for i in x if not i in punctuations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of words in caps lock in the cleaned text\n",
    "# removing the \"I\" as it is counted as a capital word but it shouldn't\n",
    "pope['num_upper_words'] = pope['cleaned'].apply(lambda x: sum([y.isupper() if y not in ['I', 'U', 'R'] else False for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting everything in lower character\n",
    "pope['final'] = pope['cleaned'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only relevant columns\n",
    "pope = pope[['date', \n",
    "           'likes',\n",
    "           'retweets',\n",
    "           'tweet',\n",
    "           'tweet_id',\n",
    "           'total_words',\n",
    "           'total_charact', \n",
    "           'hashtag', \n",
    "           'num_hashtags', \n",
    "           'mentions', \n",
    "           'num_mentions', \n",
    "           'emojis', \n",
    "           'counter_emojis',\n",
    "           'num_emojis',\n",
    "           'num_upper_words',\n",
    "           'cleaned',\n",
    "           'final'\n",
    "            ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELON MUSK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon initial dataset size: 11512\n"
     ]
    }
   ],
   "source": [
    "# loading dataset\n",
    "elon = pd.read_csv('elon_with_likes.csv')\n",
    "print('Elon initial dataset size:', len(elon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon dataset size without URL duplicates: 11288\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates with URL\n",
    "elon = elon.drop_duplicates(subset = 'tweet_id')\n",
    "print('Elon dataset size without URL duplicates:', len(elon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of words used in a tweet\n",
    "elon['total_words'] = elon['tweet'].str.split().str.len()\n",
    "\n",
    "# counting the number of characters used in a tweet\n",
    "elon['total_charact'] = elon['tweet'].apply(lambda x: len(x))\n",
    "\n",
    "# creating a column with all the hashtags used\n",
    "elon['hashtag'] = elon['tweet'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)) \n",
    "\n",
    "# creating a column with the number of hashtags used\n",
    "elon['num_hashtags'] = elon['hashtag'].apply(lambda x: len(x))\n",
    "\n",
    "# creating a column with @mentions only \n",
    "elon['mentions'] = elon['tweet'].apply(lambda x: re.findall(r'@[A-Za-z0-9]+', x))\n",
    "\n",
    "# counting the @mentions only: \n",
    "elon['num_mentions'] = elon['mentions'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the emojis, counting them and making an overall count\n",
    "elon['emojis'] = elon['tweet'].apply(lambda x: extract_emojis(x))\n",
    "elon['counter_emojis'] = elon['tweet'].apply(lambda x: split_count(x))\n",
    "elon['num_emojis'] = elon.counter_emojis.apply(lambda x : sum(x.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column with RT and @mentions\n",
    "# the reason is that user might asks his audience to retweet his tweets (e.g. \"RT this if you..\")\n",
    "\n",
    "elon['RT'] = elon['tweet'].apply(lambda x: re.findall(r'RT @(\\w+):', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Retweets: 0\n"
     ]
    }
   ],
   "source": [
    "# checking how many retweets the Pope did\n",
    "print('total Retweets:',len(elon[(elon['RT'].str.len() > 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pope dataset size without retweets: 11288\n"
     ]
    }
   ],
   "source": [
    "# dropping what appear to clearly be Retweets (RT + @mentions)\n",
    "elon = elon[(elon['RT'].str.len() == 0)]\n",
    "print('Pope dataset size without retweets:', len(elon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanind tweets from hashtags\n",
    "elon['cleaned'] = elon['tweet'].apply(lambda x: ' '.join(re.sub(r'\\B#\\w*[a-zA-Z]+\\w*',\" \", x).split()))\n",
    "\n",
    "# cleanind tweets from @mentions\n",
    "elon['cleaned'] = elon['cleaned'].apply(lambda x: ' '.join(re.sub(r'@[A-Za-z0-9]+',\" \", x).split()))\n",
    "\n",
    "# cleaning tweets from websites \n",
    "elon['cleaned'] = elon['cleaned'].apply(lambda x: ' '.join(re.sub(r'\\b(?:https?://)?(?:(?i:[a-z]+\\.)+)[^\\s,]+\\b',\" \", x).split()))\n",
    "\n",
    "# removing emojis \n",
    "elon['cleaned'] = elon['cleaned'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "# removing punctuation\n",
    "elon['cleaned'] = elon['cleaned'].apply(lambda x: ''.join([i for i in x if not i in punctuations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of words in caps lock in the cleaned text\n",
    "# removing the \"I\" as it is counted as a capital word but it shouldn't\n",
    "elon['num_upper_words'] = elon['cleaned'].apply(lambda x: sum([y.isupper() if y not in ['I', 'U', 'R'] else False for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting everything in lower character\n",
    "elon['final'] = elon['cleaned'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only relevant columns\n",
    "elon = elon[['date', \n",
    "           'likes',\n",
    "           'retweets',\n",
    "           'tweet',\n",
    "           'tweet_id',\n",
    "           'total_words',\n",
    "           'total_charact', \n",
    "           'hashtag', \n",
    "           'num_hashtags', \n",
    "           'mentions', \n",
    "           'num_mentions', \n",
    "           'emojis', \n",
    "           'counter_emojis',\n",
    "           'num_emojis',\n",
    "           'num_upper_words',\n",
    "           'cleaned',\n",
    "           'final'\n",
    "            ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGING DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column for the classification \n",
    "trump['persona'] = 'trump'\n",
    "bid['persona'] = 'biden'\n",
    "kim['persona'] = 'kim'\n",
    "pope['persona'] = 'pope'\n",
    "elon['persona'] = 'elon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump dataset size: (45077, 18)\n",
      "Biden dataset size: (6038, 18)\n",
      "Kim dataset size: (27281, 18)\n",
      "Pope dataset size: (2878, 18)\n",
      "Elon dataset size: (11288, 18)\n"
     ]
    }
   ],
   "source": [
    "# shape of raw datasets\n",
    "print(\"Trump dataset size:\", trump.shape)\n",
    "print(\"Biden dataset size:\", bid.shape)\n",
    "print(\"Kim dataset size:\", kim.shape)\n",
    "print(\"Pope dataset size:\", pope.shape)\n",
    "print(\"Elon dataset size:\", elon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Merged datasets size: (92562, 18)\n"
     ]
    }
   ],
   "source": [
    "# appending all datasets into a new df \n",
    "total = trump.append(bid) \n",
    "total = total.append(kim)\n",
    "total = total.append(pope)\n",
    "total = total.append(elon)\n",
    "print(\"Total Merged datasets size:\", total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Merged datasets size after dropping URL duplicates: (92559, 18)\n"
     ]
    }
   ],
   "source": [
    "total.drop_duplicates(subset='tweet_id', inplace = True)\n",
    "print(\"Total Merged datasets size after dropping URL duplicates:\", total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.tweet_id.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.to_csv('final_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
